*Abstract:* **serverless Next.js + shadcn/ui** AI assistant app that does **ADHD screening (not diagnosis)**, personalized coaching/planning, and **stores user history to MongoDB** with **Gemini 2.5 + NVIDIA NIM** and **round-robin API key rotation**.

---

## 0) Non-negotiables (product + safety)

1. **Positioning:** “Screening & self-understanding” — not a medical diagnosis or treatment. Your UX should say this clearly on Landing + before every assessment result.
2. **Validated screener first:** Start with **ASRS v1.1 (6Q)** as your baseline screener (free to use; cite required). ([Harvard Medical School HCP][1])
3. **Safety rails:** If users express self-harm/hopelessness, the assistant must switch to a crisis-safe flow (resources + encourage professional help).

---

## 1) Target feature set (MVP → V1)

### MVP (ship fast, high value)

* **Auth** (email+password, bcrypt, MongoDB).
* **Onboarding**: consent + goals (study/career/life) + struggles + preferred communication style.
* **ADHD Screening flow**: ASRS 6Q → optional extended questions → results explanation + “next steps”.
* **Personalized “Strength & Strategy” plan**: study/work strategies, routines, accommodations ideas.
* **Coach Chat**: ongoing Q&A, grounded in user’s saved profile + history.

### V1 “sticky” features to add

* **Planner that speaks ADHD**: task breakdown, timeboxing, reminders/checklists, “start now” micro-steps.
* **Progress insights**: triggers, best working hours, what actually helped (based on journaling + plans).
* **Knowledge retrieval**: search past sessions, plans, and chats (“what worked last time?”) using embeddings + rerank.

---

## 2) Model strategy (Gemini 2.5 + NVIDIA NIM)

### Gemini (reasoning + structured outputs)

Use Google’s official model IDs (examples):

* `gemini-2.5-flash`, `gemini-2.5-pro`, `gemini-2.5-flash-lite` ([Google AI for Developers][2])
  Typical usage:
* **Flash-Lite**: cheap “router”, classification, JSON extraction, fast UI interactions.
* **Flash**: coach chat, quick planning, summarization.
* **Pro**: heavy planning, multi-step career/study strategy, nuanced interpretation.

### NVIDIA NIM (specialized utilities + extra agents)

NIM is OpenAI-compatible for chat and supports embeddings/reranking endpoints. ([docs.api.nvidia.com][3])
Suggested roles:

* **Embeddings:** `nvidia/nv-embedqa-e5-v5` via `POST https://integrate.api.nvidia.com/v1/embeddings` ([docs.api.nvidia.com][4])
* **Reranking:** `POST https://ai.api.nvidia.com/v1/retrieval/nvidia/reranking` ([docs.api.nvidia.com][5])
* **Chat agents:** `POST https://integrate.api.nvidia.com/v1/chat/completions` ([docs.api.nvidia.com][3])
  Pick specific NIM chat models from NVIDIA’s model catalog and lock them into an “agent registry.” ([build.nvidia.com][6])

---

## 3) System architecture (serverless Next.js)

**Next.js (App Router) + Route Handlers**:

* UI in `app/(marketing)` + `app/(app)` routes
* Backend APIs in `app/api/**/route.ts` (serverless)
* “Agents” as a backend-only service layer (`src/server/agents/*`)

**Data flow**

1. User logs in → has a `userId`
2. Assessment/chat requests hit `/api/*` route
3. API route:

   * loads user profile + relevant memories from Mongo
   * selects model/agent
   * calls Gemini/NIM through a provider wrapper
   * stores result + embeds summary for retrieval later

---

## 4) Project setup

1. **Create Next app**

   ```bash
   pnpm create next-app@latest adhd-assistant --ts --tailwind --app --eslint
   cd adhd-assistant
   ```
2. **Install shadcn/ui** (official init)

   ```bash
   pnpm dlx shadcn@latest init
   ```

   ([Shadcn UI][7])
3. Add core deps

   ```bash
   pnpm add mongodb bcryptjs zod jose
   pnpm add @tanstack/react-query
   pnpm add -D @types/bcryptjs
   ```
4. Create env file `.env.local`

   ```env
   MONGO_URI=...
   GEMINI_API_1=...
   GEMINI_API_2=...
   GEMINI_API_3=...
   GEMINI_API_4=...
   GEMINI_API_5=...
   NVIDIA_API_1=...
   NVIDIA_API_2=...
   NVIDIA_API_3=...
   NVIDIA_API_4=...
   NVIDIA_API_5=...
   ```
5. Add a `src/` folder and move code there if you prefer (Cursor-friendly), or keep default.

---

## 5) MongoDB design (collections you’ll actually need)

Create these collections (you can start minimal and expand):

* `users`

  * `_id`, `email`, `passwordHash`, `createdAt`
* `profiles`

  * `userId`, goals, preferences, work/study context, self-reported struggles
* `assessments`

  * `userId`, type (`asrs6`, `asrs18`, etc.), answers, scores, interpretation, createdAt
* `chats`

  * `userId`, threadId, messages[] (or separate messages collection)
* `memories`

  * `userId`, `text`, `sourceType`, `sourceId`, `embedding[]`, `createdAt`

**Vector search:** Use MongoDB Atlas Vector Search index on `memories.embedding` (later). If you don’t want Atlas vector search, fallback is storing embeddings + doing cosine in app (slower).

---

## 6) Auth (email/password + bcrypt + JWT cookie)

### Endpoints

* `POST /api/auth/register`
* `POST /api/auth/login`
* `POST /api/auth/logout`
* `GET /api/auth/me`

### Approach

* Hash password with bcrypt
* Create JWT (`jose`) with `userId`
* Set **httpOnly** cookie `session=...`
* Add middleware to protect `/app/*` routes

(Cursor task breakdown)

1. `src/server/db.ts` – Mongo client singleton
2. `src/server/auth/password.ts` – bcrypt hash/verify
3. `src/server/auth/jwt.ts` – sign/verify JWT
4. `app/api/auth/*/route.ts` – route handlers
5. `middleware.ts` – protect app routes

---

## 7) Round-robin API key rotation (works in serverless)

Because serverless instances are stateless, implement RR using **Mongo atomic counters**:

**Collection:** `counters`

* `{ _id: "gemini", value: 0 }`
* `{ _id: "nvidia", value: 0 }`

**Function:** `nextKey("gemini")`

* `findOneAndUpdate({ _id }, { $inc: { value: 1 } }, { upsert: true, returnDocument: "after" })`
* `idx = value % 5`
* return `process.env.GEMINI_API_${idx+1}`

Add resilience:

* If request returns 429 / rate-limit, retry up to 2–3 times with `nextKey()` (no infinite loops).
* Log provider, model, latency, status (WITHOUT logging user text).

Also keep an eye on Gemini rate limits/quota behavior. ([Google AI for Developers][8])

---

## 8) Provider wrappers (clean separation)

Create a unified interface:

```ts
type LlmMessage = { role: "system"|"user"|"assistant"; content: string };

interface LlmProvider {
  chat(opts: { model: string; messages: LlmMessage[]; json?: boolean }): Promise<{ text: string; json?: any }>;
  embed?(opts: { model: string; input: string[] }): Promise<number[][]>;
  rerank?(opts: { model: string; query: string; passages: string[] }): Promise<number[]>;
}
```

### NVIDIA NIM wrapper

* Chat: `POST https://integrate.api.nvidia.com/v1/chat/completions` (OpenAI-compatible) ([docs.api.nvidia.com][3])
* Embeddings: same base URL with `/v1/embeddings` and models like `nvidia/nv-embedqa-e5-v5` ([docs.api.nvidia.com][4])
* Rerank: `POST https://ai.api.nvidia.com/v1/retrieval/nvidia/reranking` ([docs.api.nvidia.com][5])

### Gemini wrapper

Use the Gemini API model IDs from Google’s model list. ([Google AI for Developers][2])

---

## 9) Agent registry (the “multi-agent” part)

Create `src/server/agents/registry.ts`:

Example agents:

1. **RouterAgent (Gemini Flash-Lite)**
   Input: user intent (“assessment”, “planning”, “career”, “study”, “history lookup”)
   Output: JSON `{ route, confidence, needed_context }`

2. **AssessmentAgent (Gemini Flash / Pro)**
   Produces:

   * friendly explanations
   * trait hypotheses (not diagnosis)
   * “questions to ask a clinician” list
   * strategies mapped to user life

3. **QuizGeneratorAgent (Gemini Pro)**
   Generates scenario-based quizzes (“When X happens at work, what do you do?”) from user context.
   Output: strict JSON schema for UI rendering.

4. **MemoryAgent (NIM embeddings + rerank)**

   * embed summaries into `memories`
   * retrieve relevant past info for grounding answers

5. **CoachAgent (Gemini Flash)**
   Daily coaching: routines, accountability, adaptive plans.

---

## 10) ADHD screening UX (important)

### Recommended flow

1. **Consent + framing** (“screening tool, not diagnosis”)
2. **ASRS 6Q** (fast)
3. If positive or user wants deeper: **ASRS 18** + impairment questions
4. Result:

   * score + what it *means*
   * traits and common patterns
   * suggestion to seek professional evaluation if indicated
5. Save to history

ASRS scoring guidance is publicly described in common references and distributed as a screener; the Harvard page notes it’s free to use with citation. ([Harvard Medical School HCP][1])

**Key UI idea:** show results as:

* “Likelihood signal” (low/medium/high)
* “Most affected areas” (organization, sustained attention, impulsivity, emotional regulation)
* “What to try this week” (3 small experiments)

---

## 11) Retrieval + “personal history” (MongoDB + NIM)

When any assessment or plan is generated:

1. Create a short **normalized summary** (bullets)
2. Embed it with `nvidia/nv-embedqa-e5-v5` ([docs.api.nvidia.com][4])
3. Store in `memories`

When user asks something:

1. Embed the query
2. Vector search top K in Mongo
3. Rerank with NVIDIA reranker endpoint ([docs.api.nvidia.com][5])
4. Provide top snippets as context to Gemini for a grounded answer

---

## 12) UI structure (shadcn-first, calm & supportive)

Routes:

* `/` marketing
* `/auth/login`, `/auth/register`
* `/app` dashboard
* `/app/assessment` (stepper)
* `/app/chat` (coach)
* `/app/planner` (tasks + timeboxing)
* `/app/history` (assessments, plans)
* `/app/settings` (privacy, export/delete)

UX details:

* Use **steppers**, **progress**, **non-judgmental language**, **short blocks of text**, and “save & continue later”.
* Add “reduce overwhelm mode”: fewer options, larger spacing, simple wording.

---

## 13) Security & privacy checklist (especially because ADHD data is sensitive)

* Never expose API keys client-side.
* Don’t log raw user answers or chat content in server logs.
* Add “Delete my data” and “Export my data”.
* Encrypt JWT cookie + set secure flags.
* Consider field-level encryption for assessment results (optional but ideal).

---

## 14) Testing & quality gates

* Unit tests: key rotation, auth, provider wrappers
* Contract tests: JSON schemas from QuizGeneratorAgent
* E2E (Playwright): register → assessment → results saved → chat retrieval works
* Load test: concurrent calls + rate limit behavior

---

## 15) Deployment (serverless)

* Deploy Next.js to Vercel
* MongoDB Atlas (with IP allowlist as needed)
* Set env vars in Vercel
* Add basic rate limiting (can be Mongo-based per user/day if you want to avoid Redis)

---

## 16) Cursor execution plan

Follow this order so you always have a runnable system:

1. Scaffold Next + shadcn
2. Mongo connector + `users` collection
3. Auth (register/login/me) + protected routes
4. “Assessment UI skeleton” (static pages)
5. Implement ASRS 6Q flow (no AI yet) + save results
6. Add Gemini provider + AssessmentAgent explanation generation
7. Add NVIDIA embeddings + store memories
8. Add chat UI + CoachAgent (grounded by retrieval)
9. Add personalized quiz generation (JSON schema → UI renderer)
10. Add planner features + “what worked” insights from history
11. Harden: retries, rate limits, logging, safety flow, export/delete

---

## References

[1]: https://www.hcp.med.harvard.edu/ncs/asrs.php?utm_source=chatgpt.com "ASRS v1.1 Screener-English (PDF)"
[2]: https://ai.google.dev/gemini-api/docs/models?utm_source=chatgpt.com "Gemini models | Gemini API | Google AI for Developers"
[3]: https://docs.api.nvidia.com/nim/reference/create_chat_completion_v1_chat_completions_post?utm_source=chatgpt.com "Create a chat completion"
[4]: https://docs.api.nvidia.com/nim/reference/nvidia-nv-embedqa-e5-v5-infer?utm_source=chatgpt.com "Creates an embedding vector from the input text."
[5]: https://docs.api.nvidia.com/nim/reference/nvidia-reranking-4b-infer?utm_source=chatgpt.com "Create ranking"
[6]: https://build.nvidia.com/models?utm_source=chatgpt.com "Models - Try NVIDIA NIM APIs"
[7]: https://ui.shadcn.com/docs/installation/next?utm_source=chatgpt.com "Next.js - Shadcn UI"
[8]: https://ai.google.dev/gemini-api/docs/rate-limits?utm_source=chatgpt.com "Rate limits | Gemini API - Google AI for Developers"
